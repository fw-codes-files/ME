# face bounding box, point could and 3d landmarks config
3DDFA_faceBox_device: 'gpu'
3DDFA_arch: 'mobilenet' # MobileNet V1
3DDFA_widen_factor: 1.0
3DDFA_checkpoint_fp: 'weights/mb1_120x120.pth'
3DDFA_bfm_fp: 'weights/bfm_noneck_v3.pkl' # or configs/bfm_noneck_v3_slim.pkl
3DDFA_size: 120
3DDFA_num_params: 62
3DDFA_dense: True

PC_points_piars: 200
PC_points_sample_range: 38365
# CK+ dataset config
Dataset_10fold: 'dataset/CK+_10fold_samples.txt'
CK+_data_root: 'E:/cohn-kanade-images/'
CK+_dict:
  0: 'Happy'
  1: 'Angry'
  2: 'Disgust'
  3: 'Fear'
  4: 'Sad'
  5: 'Contempt'
  6: 'Surprise'
  Happy: 0
  Angry: 1
  Disgust: 2
  Fear: 3
  Sad: 4
  Contempt: 5
  Surprise: 6
CK+_split: 500
# fer2013 convolution config, forsaken
CONV_bs: 128
CONV_seed: 0
CONV_pth: './weights/best_checkpoint.tar'
# emotion-FAN convolution config, being used
FAN_type: 'self_relation-attention'
FAN_checkpoint: './weights/Resnet18_FER+_pytorch.pth.tar'
FAN_evey_fold_checkpoint:
  1: './weights/self_relation-attention_fold_12_90.9091'
  2: './weights/self_relation-attention_fold_228_92.3077'
  3: './weights/self_relation-attention_fold_312_97.5'
  4: './weights/self_relation-attention_fold_45_88.5714'
  5: './weights/self_relation-attention_fold_52_96.6667'
  6: './weights/self_relation-attention_fold_615_100.0'
  7: './weights/self_relation-attention_fold_79_96.5517'
  8: './weights/self_relation-attention_fold_813_100.0'
  9: './weights/self_relation-attention_fold_92_100.0'
  10: './weights/self_relation-attention_fold_1027_90.9091'
# PCA config
PCA_dim: 64
PCA_dir: './weights/pca.m'
# data sample strategy config
Least_frame: 20
Max_frame: 40
window_size: 10
Sample_frequency: 1
Shuffle: True
standBy: 100
selected: 20
# LSTM config
LSTM_input_dim: 204
LSTM_hidden_dim: 512
LSTM_output_dim: 7
LSTM_layerNum: 1
LSTM_cell: 'LSTM'
# Tansformer config
T_input_dim: 204
T_block_num: 6
T_head_num: 8
T_proj_dim: 2048
T_forward_dim: 2048
T_masked: True
T_activation: 'gelu'
T_output_dim: 7
T_bs_first: True
# AE config
AE_noi_percent: 0.15
AE_train_percent: 0.7
AE_loss_train: './tb/loss/AE/train/'
AE_loss_test: './tb/loss/AE/test/'
AE_mid_dim: 8
AE_pth_epoch: 6000
# ViLT config
ViLT_model_type: 2
ViLT_embedding_dim: 64
ViLT_block_num: 12
ViLT_head_num: 8
ViLT_forward_dim: 2048
ViLT_activation: 'gelu'
ViLT_output_dim: 7
ViLT_bs_first: True
# model config
use_cuda: True
batch_size: 8
learning_rate: 1e-6
epoch: 2000
fast_fold: [1,5,9]
test_fold: 5
checkpoint_pth: './'
# data Post process config
vote: False
# log information config
LOG_CHECK: '!!!ViT: 3dlms, [1,5,9] fold, MLP[cls], 8 head, learnable PE, gelu, 2000 epoch, 6 block, 2048 forward dim, 2048 input embedding, 10 ws, 8 bs, masked, shuffle True, pick up evenly !!!'
LOG_pth: 'train.log'